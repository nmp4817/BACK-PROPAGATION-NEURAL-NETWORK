# BACK-PROPAGATION-NEURAL-NETWORK
IMPLEMENTED A FULLY-CONNECTED TWO-LAYER BACK-PROPAGATION NEURAL NETWORK.

Implemented a fully-connected two-layer back-propagation neural network and trained it using <b>cifar dataset</b> by varying different parameters such as number of nodes in hidden layer, weight regularization constant, activation functions to calculate and plot <b>loss function, error rate and confusion matrix</b> of test dataset.

#### Implemented using Python and theano.
